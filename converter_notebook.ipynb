{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Morphology exporter**\n",
    "\n",
    "Hi!\n",
    "\n",
    "This notebook aims to explain the SWC to NeuroML converter I made.\n",
    "If you have any questions regarding the conversion, please contact me at s.reissenweber12@gmail.com.\n",
    "\n",
    "The converter translates the points in the SWC file to NeuroML segments, and organizes them into unbranched segment groups based on their type.\n",
    "These segment groups are then included in main segment groups (e.g. axon_group, soma_group).\n",
    "\n",
    "The converter tries to keep as much of the provided information as possible. For instance, comments at the start of SWC files, which often contain useful information, are preserved as notes at the beginning of the generated NML document.\n",
    "\n",
    "The converter also checks for SWC files that would be invalid in NeuroML and stores the error message, any additional information and fixes made in a dictionary. This can be printed if desired. When an unsolvable error arises (meaning an invalid SWC file), the conversion will stop and an exception is raised.\n",
    "\n",
    "Following is an overview of the errors it catches so far:\n",
    "- Parent ID referred to before being defined. Loops might be present\n",
    "- Line in SWC file contains an invalid amount of columns (more or less than 7)\n",
    "- SWC file does not contain any segments\n",
    "- Zero segments without parent (root segments) detected\n",
    "- More than one segment without parent (root segment) detected\n",
    "- No soma segments detected\n",
    "- Filename does not comply with neuroml filename pattern restrictions\n",
    "- Unknown type detected\n",
    "- Spherical root segment does not belong to soma_group\n",
    "- Endpoint of zero radius detected\n",
    "- Internal point of zero radius detected\n",
    "- Two segments detected with same radius and coordinates\n",
    "\n",
    "Good luck with understanding the code! :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports:\n",
    "import neuroml\n",
    "import neuroml.writers as writers\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_nml(path, output_dir=''):\n",
    "    '''\n",
    "    This function is the big function that calls all helper functions to construct the neuroml file.\n",
    "\n",
    "    Input: - path: filepath to SWC file (str)\n",
    "           - output_dir (optional): directory in which the neuroml file will be saved (str)\n",
    "\n",
    "    Returns: - name of the newly created neuroml file (str)\n",
    "             - errors: dict {error message: {occurences: int, extra_info: [str], fix: str}}\n",
    "    '''\n",
    "    \n",
    "    errors = {}\n",
    "    d, comments = open_and_split(path, errors)\n",
    "    filename = os.path.basename(path).split('.')[0]\n",
    "    filename = change_filename(filename, errors)\n",
    "    cell_ID = f\"{filename}_cell\"\n",
    "    nml_doc = neuroml.NeuroMLDocument(id=filename)\n",
    "    nml_cell = neuroml.Cell(id=cell_ID)\n",
    "\n",
    "    make_notes(comments, nml_cell)\n",
    "    n, children, type_seg, root = classify_types_branches_and_leafs(d, errors)\n",
    "    segmentGroups = find_segments(d, n)\n",
    "    nml_mor = process_segments(d, children, root, cell_ID, errors)\n",
    "    nml_cell = process_cables(segmentGroups, type_seg, nml_mor, nml_cell)\n",
    "    nml_cell = define_biophysical_properties(nml_cell, cell_ID)\n",
    "\n",
    "    nml_doc.cells.append(nml_cell)\n",
    "    \n",
    "    if output_dir:\n",
    "        nml_file = f'{output_dir}/{filename}_converted.cell.nml'\n",
    "    else:\n",
    "        nml_file = f'{filename}_converted.cell.nml'\n",
    "    writers.NeuroMLWriter.write(nml_doc, nml_file)\n",
    "\n",
    "    return nml_file.split('/')[-1], errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversionException(Exception):\n",
    "    '''\n",
    "    This is an exception class used to store the errors dictionary when an exception is raised (swc file invalid).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, message, errors):\n",
    "        super().__init__(message)\n",
    "        self.errors = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_error(errors, error_type, occurrence=1, extra_info=None, fix=None, stop=False):\n",
    "    '''\n",
    "    This function logs errors detected in the SWC file to a dictionary adds any additional information about the errors.\n",
    "\n",
    "    Input: - errors: dict {error message: {occurences: int, extra_info: [str], fix: str}}\n",
    "           - error_type: error message (str)\n",
    "           - occurence (optional): amount of occurences (int)\n",
    "           - extra_info (optional): extra information about the error (str)\n",
    "           - fix (optional): measure implemented to fix the error (str)\n",
    "           - stop (optional): should the conversion continue or not (bool)\n",
    "    \n",
    "    Returns: None\n",
    "    '''\n",
    "\n",
    "    # Check if error_type is related to unknown SWC types\n",
    "    if error_type.startswith(\"Unknown type detected\"):\n",
    "        type_id = error_type[23:]\n",
    "        if \"Unknown type detected\" not in errors:\n",
    "            errors[\"Unknown type detected\"] = {}\n",
    "\n",
    "        if type_id not in errors[\"Unknown type detected\"]:\n",
    "            errors[\"Unknown type detected\"][type_id] = {\n",
    "                \"occurrences\": 0,\n",
    "                \"fix\": None\n",
    "            }\n",
    "\n",
    "        errors[\"Unknown type detected\"][type_id][\"occurrences\"] += occurrence\n",
    "        if fix is not None:\n",
    "            errors[\"Unknown type detected\"][type_id][\"fix\"] = fix\n",
    "    else:\n",
    "        if error_type not in errors:\n",
    "            errors[error_type] = {\n",
    "                \"occurrences\": 0,\n",
    "                \"fix\": None\n",
    "            }\n",
    "\n",
    "        errors[error_type][\"occurrences\"] += occurrence\n",
    "        if extra_info is not None:\n",
    "            if \"extra_info\" not in errors[error_type]:\n",
    "                errors[error_type][\"extra_info\"] = [extra_info]\n",
    "            else:\n",
    "                errors[error_type][\"extra_info\"].append(extra_info)\n",
    "        if fix is not None:\n",
    "            errors[error_type][\"fix\"] = fix\n",
    "\n",
    "    if stop:\n",
    "        raise ConversionException(error_type, errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_split(path, errors):\n",
    "    '''\n",
    "    This function takes a (path to an) SWC file and creates a dictionary with necessary information to generate the neuroml file.\n",
    "    \n",
    "    Input: - path: filepath to SWC file (str)\n",
    "           - errors: dict {error message: {occurences: int, extra_info: [str], fix: str}}\n",
    "    \n",
    "    Returns: - d: dict {point (int): (type, x_coord, y_coord, z_coord, radius, parent)}\n",
    "             - comments: list of comments [comment (str)]\n",
    "    '''\n",
    "\n",
    "    d = {}\n",
    "    line_nr = 0\n",
    "    comments = []\n",
    "    no_par = [0, []]  # Used for checking amount of segments without parent\n",
    "    invalid_lines = []\n",
    "    soma_detected = False # Used for checking if any soma samples are present\n",
    "\n",
    "    with open(path, 'r+') as f:\n",
    "        for line in f:\n",
    "            line_nr += 1\n",
    "            if not line:\n",
    "                pass\n",
    "            elif line.startswith('#') or line.startswith(\"*\"):\n",
    "                comments.append(line[1:].strip())\n",
    "            else:\n",
    "                information = [elem for elem in line.strip().split(' ') if elem]\n",
    "                if not information:\n",
    "                    pass\n",
    "                else:\n",
    "                    if len(information) != 7:\n",
    "                        invalid_lines.append(line_nr)\n",
    "                    else:\n",
    "                        seg_ID = int(information[0]) - 1\n",
    "                        type_ID = int(information[1])\n",
    "                        x_coor = float(information[2])\n",
    "                        y_coor = float(information[3])\n",
    "                        z_coor = float(information[4])\n",
    "                        rad = float(information[5])\n",
    "                        par_ID = int(information[6]) - 1\n",
    "\n",
    "                        if par_ID > seg_ID:\n",
    "                            log_error(errors, \"Parent ID referred to before being defined. Loops might be present\", extra_info=f\"Point {seg_ID + 1}, parent {par_ID + 1}\", fix=\"No fixes. SWC file is invalid\", stop=True)\n",
    "\n",
    "                        if type_ID == 1:\n",
    "                            soma_detected = True\n",
    "\n",
    "                        if par_ID < 0:\n",
    "                            par_ID = -1\n",
    "                            no_par[0] += 1\n",
    "                            no_par[1].append(seg_ID + 1)\n",
    "\n",
    "                        d[seg_ID] = (type_ID, x_coor, y_coor, z_coor, rad, par_ID)\n",
    "        \n",
    "    # Check if there are invalid lines in the SWC file\n",
    "    if invalid_lines:\n",
    "        log_error(errors, \"Line in SWC file contains an invalid amount of columns (more or less than 7)\", occurrence=len(invalid_lines), extra_info=f\"Lines {', '.join(map(str, invalid_lines))}\", fix=\"Skipped these lines\")\n",
    "\n",
    "    # Check if cell has segments\n",
    "    if not d:\n",
    "        log_error(errors, \"SWC file does not contain any segments\", fix=\"No fixes. SWC file is invalid.\", stop=True)\n",
    "    \n",
    "    # Check if cell has more than one or zero segment(s) without a parent\n",
    "    if no_par[0] == 0:\n",
    "        log_error(errors, \"Zero segments without parent (root segments) detected\", fix=\"No fixes. SWC file is invalid.\", stop=True)\n",
    "    if no_par[0] > 1:\n",
    "        log_error(errors, \"More than one segment without parent (root segment) detected\", extra_info=f\"Points {', '.join(map(str, no_par[1]))}\", fix=\"No fixes. SWC file is invalid.\", stop=True)\n",
    "\n",
    "    # Check if cell contains soma samples\n",
    "    if not soma_detected:\n",
    "        log_error(errors, \"No soma segments detected\", fix=\"No fixes. NML file will be invalid.\")\n",
    "\n",
    "    return d, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_filename(filename, errors):\n",
    "    '''\n",
    "    This function is used to change the filename to conform to neuroml filename pattern restrictions.\n",
    "\n",
    "    Inputs: - filename: str\n",
    "            - errors: dict {error message: {occurences: int, extra_info: [str], fix: str}}\n",
    "    \n",
    "    Returns: new_name: new file name (str)\n",
    "    '''\n",
    "    \n",
    "    new_name = re.sub(r'[^a-zA-Z0-9_]', '_', filename)\n",
    "    if new_name[0].isdigit():\n",
    "        new_name = '_' + new_name\n",
    "    \n",
    "    if new_name != filename:\n",
    "        log_error(errors, \"Filename does not comply with neuroml filename pattern restrictions\", fix=f\"Changed filename to {new_name}\")\n",
    "        \n",
    "    return new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_notes(comments, nml_cell):\n",
    "    '''\n",
    "    This function creates the notes listed at the top of the neuroml file. It also includes the original comments listed in the SWC file.\n",
    "\n",
    "    Input: - comments: list of comments [comment (str)]\n",
    "           - nml_cell: neuroml cell object\n",
    "    \n",
    "    Returns: None\n",
    "    '''\n",
    "\n",
    "    nml_cell.notes = \"\\n\\n\" + '*' * 40 + \\\n",
    "                     \"\\nThis NeuroML file was converted from SWC to NeuroML format by Sietse Reissenweber's converter. \\\n",
    "                     \\nFor any questions regarding the conversion, you can email me at s.reissenweber12@gmail.com. \\\n",
    "                     \\nThe notes listed below are the notes that were originally contained in the SWC file.\\n\" \\\n",
    "                     + '*' * 40 + \"\\n\\n\"\n",
    "\n",
    "    nml_cell.notes += \"#\" * 40 + \"\\n\\n\"\n",
    "\n",
    "    for comment in comments:\n",
    "        nml_cell.notes += f'{comment}\\n'\n",
    "\n",
    "    nml_cell.notes += \"\\n\" + \"#\" * 40 + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_types_branches_and_leafs(d, errors):\n",
    "    '''\n",
    "    This function classifies the segments into different types, and determines the children of points.\n",
    "    \n",
    "    Input: - d: dict {point (int): (type, x_coord, y_coord, z_coord, radius, parent)}\n",
    "           - errors: dict {error message: {occurences: int, extra_info: [str], fix: str}}\n",
    "    \n",
    "    Returns: - n: dict {amount of children (int): [points]}\n",
    "             - children: dict {point (int): [children]}\n",
    "             - type_seg: dict {point (int): type morph. part (e.g. soma) (str)}\n",
    "             - root: point without parent (int)\n",
    "    '''\n",
    "\n",
    "    n = {0: [],\n",
    "         1: [],\n",
    "         2: []}\n",
    "    root = -float(\"Inf\")\n",
    "    children = {}\n",
    "    type_seg = {}\n",
    "    endpoints = []\n",
    "    internal_points = []\n",
    "\n",
    "    for point, info in d.items():\n",
    "        # Create dict n:\n",
    "        number_of_children = 0\n",
    "        for info2 in d.values():\n",
    "            if info2[5] == point:\n",
    "                number_of_children += 1\n",
    "        if number_of_children == 0:\n",
    "            n[0].append(point)\n",
    "        elif number_of_children == 1:\n",
    "            n[1].append(point)\n",
    "        else:\n",
    "            n[2].append(point)\n",
    "\n",
    "        # Check for 0.0 diameter:\n",
    "        if info[4] <= 0.0:\n",
    "            d[point] = info[:4] + (0.000001,) + (info[5],)\n",
    "            if point in n[0]:\n",
    "                endpoints.append(point)\n",
    "            else:\n",
    "                internal_points.append(point)\n",
    "\n",
    "        # Create dicts type_seg and types:\n",
    "        if info[0] == 1:\n",
    "            type_seg[point] = 'soma'\n",
    "        elif info[0] == 2:\n",
    "            type_seg[point] = 'axon'\n",
    "        elif info[0] == 3:\n",
    "            type_seg[point] = 'bas_dend'\n",
    "        elif info[0] == 4:\n",
    "            type_seg[point] = 'ap_dend'\n",
    "        else:  # Account for custom types\n",
    "            type_id = f'custom_{info[0]}'\n",
    "            type_seg[point] = type_id\n",
    "            log_error(errors, f\"Unknown type detected: {type_id}\", fix=f\"Added new type {type_id} and new group {type_id}_group\")\n",
    "\n",
    "        # Find root:\n",
    "        if info[5] == -1:\n",
    "            root = point\n",
    "            if type_seg[root] != 'soma':\n",
    "                log_error(errors, \"Spherical root segment does not belong to soma_group\", fix=\"No fixes. NML file will be invalid.\")\n",
    "\n",
    "        children[point] = []\n",
    "\n",
    "    # Check for endpoints with zero radius\n",
    "    if endpoints:\n",
    "        log_error(errors, \"Endpoint of zero radius detected\", occurrence=len(endpoints), extra_info=f\"Points {', '.join(map(str, endpoints))}\", fix=f\"Changed radius to small number {0.000001}\")\n",
    "    \n",
    "    # Check for internal points with zero radius\n",
    "    if internal_points:\n",
    "        log_error(errors, \"Internal point of zero radius detected\", occurrence=len(internal_points), extra_info=f\"Points {', '.join(map(str, internal_points))}\", fix=f\"Changed radius to small number {0.000001}\")\n",
    "\n",
    "    # Create dict children:\n",
    "    for point, info in d.items():\n",
    "        if point != root:\n",
    "            children[info[5]].append(point)\n",
    "\n",
    "    return n, children, type_seg, root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_segments(d, n):\n",
    "    '''\n",
    "    This function organizes the segments into unbranched segment groups of the same type.\n",
    "    \n",
    "    Input: - d: dict {point (int): (type, x_coord, y_coord, z_coord, radius, parent)}\n",
    "           - n: dict {amount of children (int): [points]}\n",
    "\n",
    "    Returns: - segmentGroups: list with lists of segmentgroups [[points], [points], ...]\n",
    "    '''\n",
    "\n",
    "    segmentGroups = []\n",
    "    \n",
    "    # Processing from leaf points to branch points:\n",
    "    for leaf in n[0]:\n",
    "        toAdd = leaf\n",
    "        group_type = d[toAdd][0]\n",
    "        segGr = []\n",
    "        segmentFound = False\n",
    "\n",
    "        while segmentFound is False:\n",
    "            if toAdd == -1:\n",
    "                segmentFound = True\n",
    "            if toAdd in n[2]:  # Found a branch point\n",
    "                segmentFound = True\n",
    "            elif d[toAdd][0] != group_type:\n",
    "                segmentGroups.append(segGr)\n",
    "                segGr = []\n",
    "                segGr.append(toAdd)\n",
    "                group_type = d[toAdd][0]\n",
    "                toAdd = d[toAdd][5]\n",
    "            else:\n",
    "                segGr.append(toAdd)\n",
    "                toAdd = d[toAdd][5]\n",
    "\n",
    "        if segGr:\n",
    "            segmentGroups.append(segGr)\n",
    "    \n",
    "    # Processing from branch points to other branch points:\n",
    "    for branch in n[2]:\n",
    "        toAdd = branch\n",
    "        group_type = d[toAdd][0]\n",
    "        segGr = []\n",
    "        segmentFound = False\n",
    "\n",
    "        while segmentFound is False:\n",
    "            if toAdd == -1:\n",
    "                segmentFound = True\n",
    "            elif toAdd in n[2] and toAdd != branch:\n",
    "                segmentFound = True\n",
    "            elif d[toAdd][0] != group_type:\n",
    "                segmentGroups.append(segGr)\n",
    "                segGr = []\n",
    "                segGr.append(toAdd)\n",
    "                group_type = d[toAdd][0]\n",
    "                toAdd = d[toAdd][5]\n",
    "            else:\n",
    "                segGr.append(toAdd)\n",
    "                toAdd = d[toAdd][5]\n",
    "\n",
    "        if segGr:\n",
    "            segmentGroups.append(segGr)\n",
    "\n",
    "    return segmentGroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_segments(d, children, root, Cell_ID, errors):\n",
    "    '''\n",
    "    This function incorporates the segments into the neuroml morphology object.\n",
    "\n",
    "    Input: - d: dict {point (int): (type, x_coord, y_coord, z_coord, radius, parent)}\n",
    "           - children: dict {point (int): [children]} \n",
    "           - root: point without parent (int)\n",
    "           - cell_ID: unique ID of neuroml cell (str) \n",
    "           - errors: dict {error message: {occurences: int, extra_info: [str], fix: str}}\n",
    "\n",
    "    Returns: nml_mor: neuroml morphology object    \n",
    "    '''\n",
    "\n",
    "    nml_mor = neuroml.Morphology(id=f'{Cell_ID}_morphology')\n",
    "\n",
    "    available_points = [root]\n",
    "    processed = []\n",
    "    all_processed = False\n",
    "\n",
    "    while all_processed is False:\n",
    "        next_to_process = min(available_points)\n",
    "\n",
    "        if next_to_process == root:  # Set distal and proximal points to root point if root\n",
    "            Soma_Root = neuroml.Point3DWithDiam(x=str(d[next_to_process][1]),\n",
    "                                                y=str(d[next_to_process][2]),\n",
    "                                                z=str(d[next_to_process][3]),\n",
    "                                                diameter=str(d[next_to_process][4] * 2))\n",
    "            distalp = Soma_Root\n",
    "            proximalp = Soma_Root\n",
    "        else:\n",
    "            distalp = neuroml.Point3DWithDiam(x=str(d[next_to_process][1]),\n",
    "                                              y=str(d[next_to_process][2]),\n",
    "                                              z=str(d[next_to_process][3]),\n",
    "                                              diameter=str(d[next_to_process][4] * 2))\n",
    "            parent = d[next_to_process][5]\n",
    "            proximalp = neuroml.Point3DWithDiam(x=str(d[parent][1]),\n",
    "                                                y=str(d[parent][2]),\n",
    "                                                z=str(d[parent][3]),\n",
    "                                                diameter=str(d[parent][4] * 2))\n",
    "\n",
    "        parentID = d[next_to_process][5]\n",
    "        if parentID != -1:\n",
    "            # Only one segment may be spherical and must belong to the soma_group SegmentGroup:\n",
    "            coord_distal = (d[next_to_process][1], d[next_to_process][2], d[next_to_process][3])\n",
    "            coord_proximal = (d[parent][1], d[parent][2], d[parent][3])\n",
    "            if coord_distal == coord_proximal and d[next_to_process][4] == d[parent][4]:\n",
    "                log_error(errors, \"Two segments detected with same radius and coordinates\", extra_info=f\"Segments {next_to_process} and {parent}\", fix=\"No fix yet\")\n",
    "            \n",
    "            segpar = neuroml.SegmentParent(segments=parentID)\n",
    "            thisSeg = neuroml.Segment(id=str(next_to_process),\n",
    "                                      name=f'Comp_{str(next_to_process)}',\n",
    "                                      distal=distalp,\n",
    "                                      parent=segpar)\n",
    "        else:\n",
    "            thisSeg = neuroml.Segment(id=str(next_to_process),\n",
    "                                      name=f'Comp_{str(next_to_process)}',\n",
    "                                      proximal=proximalp,\n",
    "                                      distal=distalp)\n",
    "\n",
    "        nml_mor.segments.append(thisSeg)\n",
    "        processed.append(next_to_process)\n",
    "\n",
    "        available_points.remove(next_to_process)\n",
    "        available_points += children[next_to_process]\n",
    "        if not available_points:\n",
    "            all_processed = True\n",
    "\n",
    "    return nml_mor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cables(segmentGroups, type_seg, nml_mor, nml_cell):\n",
    "    '''\n",
    "    This function incorporates the segment groups into the morphology object and adds them to bigger segment groups.\n",
    "    The morphology object is then added to the cell object.\n",
    "\n",
    "    Input: - segmentGroups: list with lists of segmentgroups [[point], [point], ...]\n",
    "           - type_seg: dict {point (int): type morph. part (e.g. soma) (str)}\n",
    "           - nml_mor: neuroml morphology object\n",
    "           - nml_cell: neuroml cell object\n",
    "    \n",
    "    Returns: nml_cell: neuroml cell object\n",
    "    '''\n",
    "\n",
    "    cables = []\n",
    "\n",
    "    # Create main segment groups\n",
    "    all_cables = neuroml.SegmentGroup(id='all')\n",
    "    soma_group = neuroml.SegmentGroup(id='soma_group', neuro_lex_id='SAO:1044911821')\n",
    "    axon_group = neuroml.SegmentGroup(id='axon_group', neuro_lex_id='SAO:1770195789')\n",
    "    dendrite_group = neuroml.SegmentGroup(id='dendrite_group', neuro_lex_id='SAO:1211023249')\n",
    "    basal_group = neuroml.SegmentGroup(id='basal_group', neuro_lex_id='SAO:1079900774')\n",
    "    apical_group = neuroml.SegmentGroup(id='apical_group', neuro_lex_id='SAO:273773228')\n",
    "\n",
    "    custom_groups = {}  # Dictionary to hold custom segment groups\n",
    "    counter = {}  # Dictionary to keep track of ids of groups\n",
    "\n",
    "    for segmentGroup in segmentGroups:\n",
    "        type_cable = type_seg[segmentGroup[0]]\n",
    "        if type_cable not in counter:\n",
    "            counter[type_cable] = 1\n",
    "        else:\n",
    "            counter[type_cable] += 1\n",
    "        cable_id = f'{type_cable}_{counter[type_cable]}'\n",
    "        this_cable = neuroml.SegmentGroup(id=cable_id, neuro_lex_id='SAO:864921383')\n",
    "\n",
    "        for segment in reversed(segmentGroup):\n",
    "            member = neuroml.Member(segments=segment)\n",
    "            this_cable.members.append(member)\n",
    "\n",
    "        cables.append(this_cable)\n",
    "        cable_include = neuroml.Include(segment_groups=cable_id)\n",
    "        all_cables.includes.append(cable_include)\n",
    "\n",
    "        if type_cable == 'soma':\n",
    "            soma_group.includes.append(cable_include)\n",
    "        elif type_cable == 'axon':\n",
    "            axon_group.includes.append(cable_include)\n",
    "        elif type_cable == 'bas_dend':\n",
    "            basal_group.includes.append(cable_include)\n",
    "            dendrite_group.includes.append(cable_include)\n",
    "        elif type_cable == 'ap_dend':\n",
    "            apical_group.includes.append(cable_include)\n",
    "            dendrite_group.includes.append(cable_include)\n",
    "        else:\n",
    "            custom_group_id = f'{type_cable}_group'\n",
    "            if custom_group_id not in custom_groups:\n",
    "                custom_group = neuroml.SegmentGroup(id=custom_group_id)\n",
    "                custom_groups[custom_group_id] = custom_group\n",
    "            custom_groups[custom_group_id].includes.append(cable_include)\n",
    "\n",
    "    # Append all cables and segment groups to morphology\n",
    "    for cable in cables:\n",
    "        nml_mor.segment_groups.append(cable)\n",
    "\n",
    "    for type in [all_cables, basal_group, apical_group, soma_group, axon_group, dendrite_group]:\n",
    "        if type.includes:\n",
    "            nml_mor.segment_groups.append(type)\n",
    "\n",
    "    for custom_group in custom_groups.values():\n",
    "        nml_mor.segment_groups.append(custom_group)\n",
    "\n",
    "    nml_cell.morphology = nml_mor\n",
    "\n",
    "    return nml_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_biophysical_properties(nml_cell, Cell_ID):\n",
    "    '''\n",
    "    This function defines some basic biophysical properties for the given cell.\n",
    "    \n",
    "    Input: - nml_cell: neuroml cell object\n",
    "           - Cell_ID: unique ID of neuroml cell (str) \n",
    "    \n",
    "    Returns: nml_cell: neuroml cell object\n",
    "    '''\n",
    "\n",
    "    # Create biophysical properties object\n",
    "    all_props = neuroml.BiophysicalProperties(id=f'{Cell_ID}_properties')\n",
    "\n",
    "    # Create and configure membrane properties\n",
    "    membrane_props = neuroml.MembraneProperties()\n",
    "    membrane_props.spike_threshes.append(neuroml.SpikeThresh(value='0.0 mV'))\n",
    "    membrane_props.specific_capacitances.append(neuroml.SpecificCapacitance(value='1.0 uF_per_cm2'))\n",
    "    membrane_props.init_memb_potentials.append(neuroml.InitMembPotential(value='-60.0 mV'))\n",
    "\n",
    "    # Create and configure intracellular properties\n",
    "    intra_props = neuroml.IntracellularProperties()\n",
    "    intra_props.resistivities.append(neuroml.Resistivity(value='0.03 kohm_cm'))\n",
    "\n",
    "    # Assign properties to the object\n",
    "    all_props.membrane_properties = membrane_props\n",
    "    all_props.intracellular_properties = intra_props\n",
    "\n",
    "    # Assign object to cell\n",
    "    nml_cell.biophysical_properties = all_props\n",
    "\n",
    "    return nml_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting a single file**\n",
    "\n",
    "In the second cell below, you can specify the file to be converted, as well as the output directory in which the converted file will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_file(path, output_dir):\n",
    "    '''\n",
    "    This function converts a single file to a neuroml file and saves it to an optionally specified output directory.\n",
    "    It prints a conversion message and the error dictionary.\n",
    "    '''\n",
    "\n",
    "    swc_file = os.path.basename(path)\n",
    "    try:\n",
    "        nml_file, errors = construct_nml(path, output_dir=output_dir)\n",
    "        print(f'Converted {swc_file} to the following file: {nml_file}')\n",
    "        if errors:\n",
    "            print(json.dumps(errors, indent=2, separators=(',', ': ')))\n",
    "    except ConversionException as e:\n",
    "        print(f'Error converting {swc_file}: {e}')\n",
    "        print(json.dumps(e.errors, indent=2, separators=(',', ': ')))\n",
    "    except Exception as e:\n",
    "        print(f'Error converting {swc_file}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error converting : [Errno 2] No such file or directory: ''\n"
     ]
    }
   ],
   "source": [
    "path = \"\"\n",
    "output_dir = ''\n",
    "\n",
    "convert_file(path, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting files from a directory**\n",
    "\n",
    "In the second cell below, you can specify the directory from which the swc files will be converted. Additionally, you can specify the output directory in which the converted file will be stored. The boolean print_errors will determine if the error dictionary is printed for every cell.\n",
    "\n",
    "A summary is printed, which is a dictionary containing the amount of (un)successful conversions and the amount of times certain errors occurred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from IPython.display import display, clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_directory(path_swc, path_nml, print_errors):\n",
    "    '''\n",
    "    This function converts all the SWC files in a given directory to neuroml files and saves them to an optionally specified output directory.\n",
    "    It shows the progress of the conversion and prints the error dictionaries if indicated through print_errors.\n",
    "    It prints a summary of the errors encountered and the amount of files (un)successfully converted.\n",
    "    '''\n",
    "\n",
    "    # Create dictionaries for summary of converted files\n",
    "    summary = {}\n",
    "    summary['Successful conversions'] = 0\n",
    "    summary['Unsuccessful conversions'] = 0\n",
    "    summary['Errors'] = {}\n",
    "    errors_per_file = {}\n",
    "    unsuccessful_files = []\n",
    "\n",
    "    # Use os.walk to iterate through all directories and subdirectories\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(path_swc):\n",
    "        for file in files:\n",
    "            if file.endswith('.swc'):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "\n",
    "    for i, file_path in enumerate(file_paths):\n",
    "        swc_file = os.path.basename(file_path)\n",
    "        clear_output(wait=True)\n",
    "        display(f'Converting {swc_file}... (File {i + 1}/{len(file_paths)})')\n",
    "\n",
    "        try:\n",
    "            nml_file, errors = construct_nml(file_path, output_dir=path_nml)\n",
    "            summary['Successful conversions'] += 1\n",
    "        except ConversionException as e:\n",
    "            errors = e.errors\n",
    "            summary['Unsuccessful conversions'] += 1\n",
    "            unsuccessful_files.append(swc_file)\n",
    "            display(f'Error converting {swc_file}: {e}')\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            display(f'Error converting {swc_file}: {e}')\n",
    "            time.sleep(2)\n",
    "        \n",
    "        if print_errors and errors:\n",
    "            errors_per_file[swc_file] = errors\n",
    "\n",
    "        for error in errors:\n",
    "            if error not in summary['Errors']:\n",
    "                summary['Errors'][error] = 1\n",
    "            else:\n",
    "                summary['Errors'][error] += 1\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display('Conversion complete!')\n",
    "    print(\"Summary:\")\n",
    "    pprint.pprint(summary)\n",
    "    if unsuccessful_files:\n",
    "        print(\"The conversion was unsuccessful for files\", unsuccessful_files)\n",
    "\n",
    "    if print_errors:\n",
    "        print(\"\\nErrors per file:\")\n",
    "        for file, errors in errors_per_file.items():\n",
    "            print(f\"{file}: {json.dumps(errors, indent=2, separators=(',', ': '))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_swc = \"swc_no_api\"\n",
    "path_nml = 'nml_no_api'\n",
    "print_errors = False\n",
    "\n",
    "convert_directory(path_swc, path_nml, print_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neuromorpho API**\n",
    "\n",
    "The code below is used to access the API of neuromorpho.org, which contains a large database of SWC files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from neuromorpho_api import requestor as requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_swc_file(neuron_id):\n",
    "    '''\n",
    "    This function fetches the information about the specified neuron id and fetches the corresponding SWC file using the generated url.\n",
    "\n",
    "    Input: neuron_id: id of neuron on neuromorpho.org (int)\n",
    "\n",
    "    Returns: - SWC file contents (bytes)\n",
    "             - swc_name: name of swc file (str)\n",
    "    '''\n",
    "\n",
    "    endpoint = \"https://neuromorpho.org/api/\"\n",
    "    start = time.time()\n",
    "    response = requests.get(endpoint + f\"neuron/id/{neuron_id}\")\n",
    "    print(data.keys())\n",
    "    if response.status_code != 200:\n",
    "        raise(\"Failed to fetch SWC file:\", response.text)\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    # Construct and fetch the SWC URL\n",
    "    swc_url = f\"https://neuromorpho.org/dableFiles/{data['archive'].lower()}/CNG%20version/{data['neuron_name']}.CNG.swc\"\n",
    "    swc_name = data['neuron_name']\n",
    "\n",
    "    if time.time() - start < 1/3:\n",
    "        time.sleep(1/3 - (time.time() - start))\n",
    "        \n",
    "    swc_response = requests.get(swc_url)\n",
    "    \n",
    "    if swc_response.status_code != 200:\n",
    "        raise(\"Failed to fetch SWC file:\", swc_response.text)\n",
    "    \n",
    "    time.sleep(1/3)\n",
    "\n",
    "    return swc_response.content, swc_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_swc_file(neuron_id, output_dir=''):\n",
    "    '''\n",
    "    This function writes the SWC contents to a new SWC file in an optionally specified output directory.\n",
    "\n",
    "    Input: - neuron_id: id of neuron on neuromorpho.org (int)\n",
    "           - output_dir (optional):  directory in which the SWC file will be saved (str)\n",
    "    \n",
    "    Returns: name of the newly created neuroml file (str)\n",
    "    '''\n",
    "    start_fetch = time.time()\n",
    "    swc_content, swc_name = fetch_swc_file(neuron_id)\n",
    "    fetch_time = time.time() - start_fetch\n",
    "    \n",
    "    start_write = time.time()\n",
    "    if swc_content:\n",
    "        if output_dir:\n",
    "            with open(f\"{output_dir}/{swc_name}.swc\", \"wb\") as f:\n",
    "                f.write(swc_content)\n",
    "            write_time = time.time() - start_write\n",
    "            return f\"{output_dir}/{swc_name}.swc\", fetch_time, write_time\n",
    "        \n",
    "        else:\n",
    "            with open(f\"{swc_name}.swc\", \"wb\") as f:\n",
    "                f.write(swc_content)\n",
    "            write_time = time.time() - start_write    \n",
    "            return f\"{swc_name}.swc\", fetch_time, write_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Converting files from API**\n",
    "\n",
    "In the second cell below, you can specify what range of cells you want to fetch from the API, as well as the output directories in which the SWC and NML files will be stored. Additionally, print_errors determines if the error dictionary is printed for every cell. \n",
    "\n",
    "A summary is printed, which is a dictionary containing the amount of (un)successful conversions and the amount of times certain errors occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_api(range_api, output_dir_swc, output_dir_nml, print_errors):\n",
    "    '''\n",
    "    This function fetches the neurons given by range_api from the neuromorpho API and converts the fetched SWC files to neuroml files.\n",
    "    It saves them to an optionally specified output directory.\n",
    "    It shows the progress of the conversion and prints the error dictionaries if indicated through print_errors.\n",
    "    It prints a summary of the errors encountered and the amount of files (un)successfully converted.\n",
    "    '''\n",
    "\n",
    "    # Create dictionaries for summary of converted files\n",
    "    summary = {}\n",
    "    summary['Successful conversions'] = 0\n",
    "    summary['Unsuccessful conversions'] = 0\n",
    "    summary['Errors'] = {}\n",
    "    unsuccessful_files = []\n",
    "    errors_per_file = {}\n",
    "    fetch_time = []\n",
    "    conversion_time = []\n",
    "\n",
    "    for i, neuron_id in enumerate(range(*range_api)):\n",
    "        try:\n",
    "            clear_output(wait=True)\n",
    "            display(f'Fetching neuron {neuron_id}... (File {i + 1}/{len(range(*range_api))})')\n",
    "\n",
    "            path, fetch_time, write_time = create_swc_file(neuron_id, output_dir=output_dir_swc)\n",
    "\n",
    "            swc_file = os.path.basename(path)\n",
    "            clear_output(wait=True)\n",
    "            display(f'Converting {swc_file}... (File {i + 1}/{len(range(*range_api))})')\n",
    "\n",
    "            try:\n",
    "                start_conversion = time.time()\n",
    "                nml_file, errors = construct_nml(path, output_dir=output_dir_nml)\n",
    "                conversion_time.append(time.time() - start_conversion)\n",
    "                summary['Successful conversions'] += 1\n",
    "            except ConversionException as e:\n",
    "                errors = e.errors\n",
    "                summary['Unsuccessful conversions'] += 1\n",
    "                unsuccessful_files.append(swc_file)\n",
    "                display(f'Error converting {swc_file}: {e}\\n')\n",
    "                time.sleep(2)\n",
    "            except Exception as e:\n",
    "                display(f'Error converting {swc_file}: {e}\\n')\n",
    "                time.sleep(2)\n",
    "\n",
    "            if print_errors and errors:\n",
    "                errors_per_file[swc_file] = errors\n",
    "\n",
    "            for error in errors:\n",
    "                if error not in summary['Errors']:\n",
    "                    summary['Errors'][error] = 1\n",
    "                else:\n",
    "                    summary['Errors'][error] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            if 'Unsuccessful fetch' not in summary['Errors']:\n",
    "                summary['Errors']['Unsuccessful fetch'] = 1\n",
    "            else:\n",
    "                summary['Errors']['Unsuccessful fetch'] += 1\n",
    "            display(f\"Unsuccessful fetch for neuron {neuron_id}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display('Conversion complete!')\n",
    "    print(\"Summary:\")\n",
    "    pprint.pprint(summary)\n",
    "    print(f\"\\nAverage fetching time: {np.mean(fetch_time)}\")\n",
    "    print(f\"Average writing time: {np.mean(write_time)}\")\n",
    "    print(f\"Average conversion time: {np.mean(conversion_time)}\")\n",
    "    \n",
    "    if unsuccessful_files:\n",
    "        print(\"The conversion was unsuccessful for files\", unsuccessful_files)\n",
    "    \n",
    "    if print_errors:\n",
    "        print(\"\\nErrors per file:\")\n",
    "        for file, errors in errors_per_file.items():\n",
    "            print(f\"{file}: {json.dumps(errors, indent=2, separators=(',', ': '))}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conversion complete!'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "{'Errors': {'Filename does not comply with neuroml filename pattern restrictions': 200},\n",
      " 'Successful conversions': 200,\n",
      " 'Unsuccessful conversions': 0}\n",
      "\n",
      "Average fetching time: 1.7500789165496826\n",
      "\n",
      "Average writing time: 0.0010101795196533203\n",
      "Average conversion time: 0.03451461911201477\n"
     ]
    }
   ],
   "source": [
    "range_api = (1, 2)\n",
    "output_dir_swc = 'swc_api'\n",
    "output_dir_nml = 'nml_api'\n",
    "print_errors = False\n",
    "\n",
    "convert_api(range_api, output_dir_swc, output_dir_nml, print_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
